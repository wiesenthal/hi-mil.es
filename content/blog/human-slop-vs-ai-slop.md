---
createdAt: 2025-06-26
updatedAt: 2025-06-26
---

# Epistemic Urgency Distinguishes Human Slop from AI Slop

There is a difference between content written by a human and content written by an LLM.

What is it?

It is not necessarily the _quality_ of the writing. Some debate that there will always be a creative element that is essential to human writing which LLMs can never truly replicate.

I have not heard a logical argument for why this is the case, and I see no reason to believe it, other than as a soothing mechanism for fear of human redundancy.

For a given instruction, e.g. "write a book about orchids", it's likely that today's LLMs will produce a better book than the average human, and tomorrow's LLMs a better book than the leading human orchid expert.

No, the difference I notice is an epistemic one, not a qualitative one.

##### _Epistemic: relating to knowledge or the process of knowing._

That is - knowledge created by an LLM is not urgent, whereas knowledge created by a human is urgent to the moment it is created.

Let me explain.

There are three variables that affect the output of an LLM:

- The weights (the billions of numbers which compose the models representation of the world)
- The random seed
- The prompt

Under fixed weights and seed, if you give an LLM the same prompt it will generate the same output **every time**.

Unless you are training the model, an LLM's weights are frozen while it is being used.

The random seed can be waved away. The seed is a number which adds variance to the next sampling from the model's distribution of all possible words. It doesn't add useful information so you can use the same random seed every time.

In practice, the LLM's output is sensitive only to one variable: **the prompt**.

And an LLM is always in this state. When it outputs the first word of its response, the model is not aware that it is now mid-response. The word it just created is added to the prompt, and the process repeats. An LLM is always starting from what it has. You could interrupt it mid-sentence for a thousand years, and when resumed it will carry on the train of thought that is now embedded in its prompt.

This determinism is actually what makes LLMs such a powerful tool. Once you choose a prompt, all variables are controlled for. The results are interpretable. If an LLM performs poorly, we know that they didn't just wake up on the wrong side of the bed today. We can compare different prompts side-by-side. We can replicate experimental results.

Given certain input to an LLM, the output is certain.

Therefore, for any prompt, there is no urgency to generate the LLM's response.

Of course, if you are are using it for work or another purpose you will generate it when you need it.

Epistemically, however, there is no need. The knowledge produced by pressing that generate button could be produced any time, or any place. It is more like the knowledge has already been produced, and it is simply waiting to be accessed.

This is in contrast to how a human produces output given input.

Take this very blog post for example (I am human, after all).

I am not writing in a vacuum. I started with a kind of mental prompt: wanting to write about this difference between human and AI text.  
Alternatively you could've given me a prompt, such as on an exam.

My prompt certainly is a huge factor in my output, perhaps the largest one. However, it is not the only input. And my "weights" (neurons) are always in flux.

Everything is my input. The feeling in my neck, my breakfast digesting in my gut, my childhood memories, each billboard I saw on the I-80 conveying a different message about a different AI product â€” you get the idea.

You could try to catalog all of the input variables to the language center of my brain, but, you'd be wrong. _Everything_ factors in.

The brain is a chaotic system plugged into a larger chaotic system. Chaos means tiny differences in initial conditions lead to drastically different results.

It's the **butterfly effect**. A butterfly flapping it's wing across the hemisphere could cause a hurricane (that I see on the news, not experience because I live in California of course), which causes me to choose a single word differently, based on one I was reminded of by the news article.

On the other hand, even if that hurricane _took down the datacenter_ hosting the LLM inference, the hurricane would have zero impact on its output. Just give the same prompt (and seed) to the same model hosted somewhere else, and you'll get the same output.

Perhaps its our sensitivity to the total universe in the present moment that imbues human art with its invaluable nature.

Can AI system could achieve epistemic urgency by integrating realtime and seemingly arbitrary data into its input?  
Well, not unless it incorporated some kind of analog processing into its cognition (like a petri dish of neurons). Because AI is encoded on digital computers, any such input must be quantized into a series of 0's or 1's. That series could be easily copied, as opposed to a continous analog signal which could never be _perfectly_ replicated.  
Plus, if it did achieve epistemic urgency that would undermine its utility as a research tool.

It may be this difference that defines the intelligence as artificial. It is not that the intelligence is unreal, it is that it operates in a discretized realm.

The value of having access to _both_ kinds of intelligence strikes me.  
And so my sense is that these thinking machines will continue as partners to the human biocomputing brains. Together we'll improve the interface between the continuous and the discrete, metaphorically (or phyiscally) merging as one intelligence.

**Superintelligence** lies not in artificial intelligence, nor in biological intelligence, but between the two.
