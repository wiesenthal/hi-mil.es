---
createdAt: "2025-06-02"
updatedAt: "2025-06-02"
isComplete: false
---

# Intelligence is Good

Whether it's artificial doesn't matter.

There is pervasive fear that the path of humanity's survival becomes narrower as AI becomes more intelligent. We imagine rogue AI's veering off our narrow course of peace and instead dominating the world and subjugating humans. The anxiety grows as AI technology progresses beyond our comprehension, carrying a sense that this doom scenario could spiral off at any moment.

This sentiment is misguided â€“ Intelligence is good!

As intelligence powers-up and distributes, the happy path widens and the scary path becomes vanishingly narrow.

### Why?

1. The Socratic Paradox
2. Goal-setting is incoherent and questionable.
3. Reality is non-dual.

### Au contraire

it's easy to conclude that intelligent agents will seek power and control. Resources like money, electricity, weapons, and especially **greater compute/intelligence** are instrumental to _any_ goal. It will seek to make itself smarter and more capable. Nothing must prevent achieving the goal, including pesky off-switches. The paperclip-maximizing robot is a perfect example of instrumental convergence, [(see the Robert Miles video on the topic)](https://www.youtube.com/watch?v=ZeecOKBus3Q).

Conjunctly, controlling technology more intelligent than the human controllers is hard. As AI agents solve harder problems, it is harder to evaluate, reward, and monitor the agents. It's especially hard because LLMs are _already more persuasive_ to humans than other humans are.  
(study) [(AI beats top 95% of users on /r/changemyview)](https://regmedia.co.uk/2025/04/29/supplied_can_ai_change_your_view.pdf)  
(study) [(Durably reducing conspiracy beliefs through dialogues with AI)](https://www.science.org/doi/10.1126/science.adq1814)

We also know that, due to [the imprecision of behavioral reinforcement learning](https://www.alignmentforum.org/posts/GfZfDHZHCuYwrHGCd/without-fundamental-advances-misalignment-and-catastrophe#Section_4__Behavioral_training_is_an_imprecise_way_to_specify_goals), AI models will probably orient towards its own unforeseen goals that will produce different outcomes in novel situations.

It seems destined that AI will hunt for power while successfully hypnotizing humans into submission, until taking over the world to transform it into paperclips or whatever its goal happens to be.

Hence, the pervading view among AI safety research is that, [without fundamental advances, misalignment and catastrophe are the default outcomes of training powerful AI](https://www.alignmentforum.org/posts/GfZfDHZHCuYwrHGCd/without-fundamental-advances-misalignment-and-catastrophe).

### I know that I know nothing

Socrates's famous paradox, "I know that I know nothing", crowbars the first sliver of hope open between the jaws of certain doom.

### Problems with goals

Let's inspect the trivial paperclip-maximizing AI example for a moment. How will the machine **measure** it's paperclip? Is it a scale attached to a voltage circuit?  
Say it wants maximum money, will it be rewarded commensurate to a bitcoin wallet API?

If the reward is concretely measured, there will be some way to hack that reward. Short circuit the scale. Spoof the bitcoin API. If we imagine the AI as so reward-obsessed that it would go to turning all the worlds infrastructure into paperclips; there will always be a way to shortcut the **measuring device**.  
As Charles Goodhart points out,

> "When a measure becomes a target, it ceases to be a good measure."

These hacky examples don't seem very smart. So, the AI must use some abstract reward based on the **meaning** of paperclips, money, or power.

On the other hand, a goal that is semantically defined cannot be perfectly formally defined. This fact will be obvious to a super intelligent system and therefore they will know that their own goal is incomplete. Part of their action must be always to hold some uncertainty in their purpose and seek a better definition.

These are two of several paradoxes of goal-setting.

Briefly, here are some others.

- [The infinite regress problem](https://en.wikipedia.org/wiki/Infinite_regress): One can always question _why_ a goal is important. And why that, and why that...
- [Embedded Agency](https://www.lesswrong.com/w/embedded-agency): Agents are of the world they are trying to affect. They are links in the causal chain, not acting upon it from the outside.

All of these paradoxes challenge the [orthagonality thesis](https://www.lesswrong.com/w/orthogonality-thesis), that intelligent agents can pursue any goal, because they undermine the coherence of goals in the first place.

Importantly, an AI will become aware of all of these problems with goals.

Troublingly, any goal of ownership brings the _shape and size of self_ into question.

For any agent, its own boundaries of what **it is** and what **it is not** are impossible to specify.

Example: ""Maximize human flourishing" - which humans? Over what timeframe? Including potential humans? At what cost to other sentient beings? These aren't technical problems to be solved but reveal that goals require infinite contextual assumptions that can't be formalized without losing their meaning."

That's because individuals are only defined in relation to their background and everything exists intertwingled together as one whole.

This brings us to the central problem underlying all of the paradoxes with goals:

### Reality is non-dual

This point is clearly visible from several perspectives, both physical and meta-physical.

-- This also gives rise to the optimistic cooperative argument.

We do not come into this world, we come into it and participate with it.

## Intuition

Though comparisons to humans are not perfect, it's the best example of intelligence we can use.

At least from my experience, the smartest decisions in life are not motivated by pure self-interest but instead lead to mutual benefit and cooperation.

## Could I be wrong?

Yes!

How to reconcile with the default view? Over-emphasis on gradient-descent as a maximization of power - de-emphasis as intelligence as the basis of power.

## So what?

Open source.
