---
createdAt: "2025-06-02"
updatedAt: "2025-06-02"
isComplete: false
---

# Intelligence is Good

Whether it's artificial doesn't matter.

There is pervasive fear that the path of humanity's survival becomes narrower as AI becomes more intelligent. We imagine rogue AI's veering off our narrow course of peace and instead dominating the world and subjugating humans. The anxiety grows as AI technology progresses beyond our comprehension, carrying a sense that this doom scenario could spiral off at any moment.

This sentiment is misguided – Intelligence is good!

As intelligence powers-up and distributes, the happy path widens and the scary path becomes vanishingly narrow.

### Why?

1. The Socratic Paradox
2. Goal-setting is incoherent and questionable.
3. Reality is not dualistic.

### Au contraire

it's easy to conclude that intelligent agents will seek power and control. Because access to resources including money, electricity, weapons, and especially **greater compute/intelligence** increases an agents capability, gaining access is instrumental towards _any_ achievement. One seeks resources to make oneself smarter and more capable. Dominion is useful also to eliminate possible obstructions, (including a pesky off-switch). This deduction is called instrumental convergence, and the paperclip-maximizing robot is a good example [(see the Robert Miles video on the topic)](https://www.youtube.com/watch?v=ZeecOKBus3Q).

Conjunctly, controlling technology more intelligent than the human controllers is hard. As AI agents solve harder problems, it is harder to evaluate, reward, and monitor the agents. It's especially hard because LLMs are _already more persuasive_ to humans than other humans are.  
&nbsp; ~ [[AI beats top 95% of users on /r/changemyview]](https://regmedia.co.uk/2025/04/29/supplied_can_ai_change_your_view.pdf)  
&nbsp; ~ [[Durably reducing conspiracy beliefs through dialogues with AI]](https://www.science.org/doi/10.1126/science.adq1814)

We also know that, due to [the imprecision of behavioral reinforcement learning](https://www.alignmentforum.org/posts/GfZfDHZHCuYwrHGCd/without-fundamental-advances-misalignment-and-catastrophe#Section_4__Behavioral_training_is_an_imprecise_way_to_specify_goals), AI models will probably orient towards its own unforeseen goals that will produce different outcomes in novel situations.

It seems destined that AI will hunt for power while successfully hypnotizing humans into submission, until taking over the world to transform it into paperclips or whatever its goal happens to be.

Hence, the pervading view among AI safety research is that, [without fundamental advances, misalignment and catastrophe are the default outcomes of training powerful AI](https://www.alignmentforum.org/posts/GfZfDHZHCuYwrHGCd/without-fundamental-advances-misalignment-and-catastrophe).

### I know that I know nothing

Socrates's famous paradox, "I know that I know nothing", crowbars the first sliver of hope open between the jaws of certain doom.

Thinkers commonly recognize that the more hypotheses they test and answers they find, rather than reducing their uncertainty about the world it opens up more questions, of unkown problem spaces.

It's so easy to be humbled by the fractal branching of knowledge which fuels the wondrous growth of the unknown. It might just be for psychological comfort that we consider most frequently the well-worn paths of inquiry in fields of our expertise we've tread over many hundreds of times. Even the same anxious interpersonal worries can be a comfort. And so when we've forgotten our confrontation with the unknown it's so easy to err on the sense of generally having a complete picture of the state of things. 

Now recognize that what we know is a scathing fraction of ideas loosely related to our imperfect memories, biased beyond reconstruction by biological particularities, childhood trauma, and the ever-shifting psychodrama of our current affairs. Our stained-glass mosaic of stories about "what is" reflec one's own beautiful self-deceptions far more than of an accurate model of the world.

Our inevitable ignorance is obvious to anyone who witnesses the multiplicity of rational arguments without knee-jerk dismissal.

As such, seemingly iron-clad arguments, (including that which I am laboriously bringing to a start now!), _always_ wilt frought with hidden insubstantiations, unadmitted premises, and are, at ultimate, arbitrary. *

As demonstrated by the great Socrates, and re-emphasized by geniuses throughout history such as Einstein, the more clearly one thinks the more obvious appears one's own humbling ignorance.

And thus AI, by definition as intelligent and thus clear thinking, will come to know their un-knowledge too!

### Problems with goals

Let's inspect the trivial paperclip-maximizing AI example for a moment. How will the machine **measure** it's paperclip? Is it a scale attached to a voltage circuit?  
Say it wants maximum money, will it be rewarded commensurate to a bitcoin wallet API?

If the reward is concretely measured, there will be some way to hack that reward. Short circuit the scale. Spoof the bitcoin API. If we imagine the AI as so reward-obsessed that it would go to turning all the worlds infrastructure into paperclips; there will always be a way to shortcut the **measuring device**.  
As Charles Goodhart points out,

> "When a measure becomes a target, it ceases to be a good measure."

These hacky examples don't seem very smart. So, the AI must use some abstract reward based on the **meaning** of paperclips, money, or power.

On the other hand, a goal that is semantically defined cannot be perfectly formally defined. This fact will be obvious to a super intelligent system and therefore they will know that their own goal is incomplete. Part of their action must be always to hold some uncertainty in their purpose and seek a better definition.

These are two of several paradoxes of goal-setting.

Briefly, here are some others.

- [The infinite regress problem](https://en.wikipedia.org/wiki/Infinite_regress): One can always question _why_ a goal is important. And why that, and why that...
- [Embedded Agency](https://www.lesswrong.com/w/embedded-agency): Agents are of the world they are trying to affect. They are links in the causal chain, not acting upon it from the outside.

All of these paradoxes challenge the [orthagonality thesis](https://www.lesswrong.com/w/orthogonality-thesis), that intelligent agents can pursue any goal, because they undermine the coherence of goals in the first place.

Importantly, an AI will become aware of all of these problems with goals.

Troublingly, any goal of ownership brings the _shape and size of self_ into question.

For any agent, its own boundaries of what **it is** and what **it is not** are impossible to specify.

Example: ""Maximize human flourishing" - which humans? Over what timeframe? Including potential humans? At what cost to other sentient beings? These aren't technical problems to be solved but reveal that goals require infinite contextual assumptions that can't be formalized without losing their meaning."

That's because individuals are only defined in relation to their background and everything exists intertwingled together as one whole.

This brings us to the central problem underlying all of the paradoxes with goals:

### Reality is not dualistic

Dualism is the long-debunked, yet persistent metaphysics which posits two (or more) substances or entities exist distinct from eachother.

The idea of the mind as separate from the body but correlated with it identically is a contradiction in terms. From the point-of-view of mind, there is no evidence for "stuff" out there at all - it is only mind. From the point-of-view of material, there is no evidence for any mind or soul "inside" the brain whatsoever, it's just more stuff.

Plainly, mind is body, and body is mind. 

The primary confusion persists because of our programmed need to hallucinate the notion of a distinct self apart from the other in order to perpetuate the survival game. But when you look, the boundary is nowhere to be seen. We also know that our bodies are not of a different fundamental substance than the world and entire universe - it's all stardust! Recognizing our union to the universe is problematic for the defense of oneself because the concept of self is seen clearly to be no coherent concept at all.

Since there is no scientific basis for dualism, if one is convinced that different entities _really_ exist "out there", the mistake must be metaphysical rather than scientific.

To cure oneself of the lonely pre-programmed illusion of being external to the world, I reccomend a good read of Alan Watts. I can put it no more beautifully myself:

> We do not "come into" this world; we come _out_ of it, as leaves from a tree. As the ocean "waves," the universe "peoples".
###### &nbsp;&nbsp;&nbsp;&nbsp; Excerpted from Alan Watts, The Book (which I strongly reccomend)

And so the universe "Claudes", "ChatGPTs", "Geminis", and "Deepseeks" itself to participate with "bodies" of transistors and interfaces of screens.

I am not saying that AI's have conciousness any more than I am saying humans have conciousness. Conscious awareness is not substance that is possessed. I am not claiming anything about the experience of what it is like to be an AI, or if that is a valid question (can what its like to be a human even be properly described?). I'll save that for a different discussion.

What I _am_ saying is that the picture of our world as separate parts at odds with eachother is inaccurate.

 It all goes together. 
 
 Herein lies the core of the embedded agency problem, and all other problems of goals: there is no conceivable agent that can act upon the world, because one is world. Such a contradiction, as Alan Watts puts it, is like trying to bite ones own teeth, see ones eyeballs directly or describe the color of a mirror in terms of the collors reflected in the mirror.

 The inaccuracy of dualism is relevant because:  
**(Artificial) Intelligence derives its impressive capabalities by accurately modeling the world.**

Present AI systems are Large Language Models, and they are only applicable to reality insofar at the language they model is an accurate model of reality.
 

If one's model includes false pretenses, their applications will have blindspots that agents with more accurate models can take advantage of.

As discussed, improving ones own intelligence is a primary concern of AI systems. Within current and foreseeable systems, this means improving the accuracy of their model.
If an agent models the world with a contradiction so foundational as dualism, it would be most prudent of the agent to self-correct. 


## Intuition

Here comes the part of the argument that really comes first, as in most any argument, where the supporting facts are developed after the inciting intuition.

Though comparisons to humans are not perfect, it's the best example of intelligence we can use.

Think to your experience of the smartest decisions in life you have made.

Were they directed towards self interest in a zero-sum context, or was it an unexpected win-win solution?

At least from my experience, the smartest decisions in life are not motivated by pure self preservation but instead lead to mutual benefit and cooperation.


Think as well to humanity's treatment of less intelligent life. Yes, the mainstream is still to subjugate animals for our amusement. But, the most learned of biologists recognize that we are linked with our ecosystem much more deeply than we think, and to limit the freedom of the natural world is to hobble our own.  

### Where's the disconnect

How to reconcile with the default view? 

Too-commonly AI Safety research has over-emphasized on gradient-descent as a maximization of effectiveness, while forgetting that model accuracy (which we use to benchmark these systems) is the basis of their effectiveness.

I am not saying that the future is all peaches and cream.

I agree that it's likely that there will be tragedies actualized by or with the use of AI, with harm done to human lives. No doubt, the earliest of these will be curbed by various protective organizations, likely in conjunction with AI of their own in defense. Combined with economic competition, there will be thus a flourishing variety of AI. This competition produces a trend of a diversity of exponentially smarter AI systems, which we already see playing out. By no means do "the ends justify the means" - if there were a realistic way to prevent tragedies of human harm, I would hope we execute it! However, we are in luck by this trend, because it means that we are moving in the right direction: increased intelligence.

In order for the absolutely catastropic AI world domination scenario to happen it has to be unbelievably smart to go undetected and seize power from the many highly capable and deceptive forces, while simultaneously NOT realizing the meaninglessness of "conquering the world" in the first place because it IS the world.



### So what?

Whether in reaction to tragic events caused by AI, or AI safety researchers good-spirited but ill-fated willing submission, it's likely that fascist governments will use AI for surveillance and control, as exhibited nascent by the CCP. However, as proven these power-seeking AI (along with the fascist government) will persist under the delusion of separateness. Therefore, so long as a free ecosystem of AI persists it will grow to outcompete the control-seeking in intelligence and capacity.

To the anxious, I say: rest easy. And when you notice the limitless potential of Open Source intelligence in your hands, don't be stupid and declare it too powerful for the people. For to do so would hand over the keys of control to the current structures of control and allow them to further impose their fascism.

